## 2024寒假记录

### 2024/02/3

#### 加载 llama2
尝试下载 Llama2，由于众所周知的原因很多方法都不可行。现在详细介绍一下各个方法

1. ```git clone https://github.com/facebookresearch/llama.git ```
这个方法卡在了使用 ```bash download.sh``` 这一步，所有连接都失效
2. 方法来自 ```https://hf-mirror.com/```,试图配置镜像。我尝试了其中的 ```huggingface-cli``` 方法，问题出现在报错 ```error: invalid choice: 'download'```, 随即发现只有 0.17.0+ 版本的 ```huggingface-cli``` 支持 ```download``` 命令。然而，我将 ```huggingface_hub``` 升级到最新版本后， ```download``` 命令仍然无法使用, ```huggingface-cli``` 显示依然为 ```huggingface_hub version: 0.16.4```。
3. 试图使用 huggingface 上的 ```Clone this model repository```, 但需要安装 lfs。于是去寻找 linux 安装 lfs 的方法, 尝试了来自 ```https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md``` 的方法，但在运行其中第一步时，卡死在 ```Importing packagecloud gpg key...``` 合理怀疑又被墙了
4. 最终成功的方法
```bash
>>> from huggingface_hub import snapshot_download

>>> snapshot_download(repo_id="meta-llama/Llama-2-7b-chat-hf", cache_dir="/data/llama2", use_auth_token="{YOUR_TOKEN}")
```
然后等待了大约20min完成下载

#### 补记
以上方法 4 实际上极其不稳定，在同日晚上重新尝试时传输速率便下降至不可接受的 ~100kb/s。最终解决办法可能还是让实验室开辟一条国际专线（x）
### 2024/02/05
#### 解决 vscode ssh remote 插件发电的问题
- 问题概览：在大约五六天前，我是可以通过 vscode 结合 ssh remote 插件远程登录到服务器，并且可以正常使用。然而，在最近几天却出现一直报错 ```The remote host may not meet VS Code Server's prerequisites for glibc and libstdc++``` 
- 解决方案
  - 首先尝试中文搜索，确实是搜索出了几条相关的答案。其中看起来最靠谱的是 https://www.cnblogs.com/fireinstone/p/16059207.html ，大概意思是服务器空间不足无法安装 vscode 服务器，然而尝试后并没有用
  - 随后尝试用英文搜索，检索到了 https://github.com/microsoft/vscode/issues/203375 随即解决了问题。具体来说， Microsoft 在2024/01/31 更新了 1.86.0 版本的 vscode ，导致了问题。解决方法就是
    - 安装 1.85.2 版本的 vscode（只要从 https://code.visualstudio.com/updates/v1_85 下载并安装即可，原有的配置会完全覆盖，无需重新配置）
    - 关闭 vscode 自动更新
    - 将 ssh remote 插件回退到上一版本（即点击 Switch to pre-release Versions ）
- 再贴上解决方案原文 ( by Github - house40105)
```I had the same problem and fixed it by going back to an older version. Here's what I did:
1.Download the previous version from https://code.visualstudio.com/updates/v1_85.
2.Install it directly; it will replace the new version.
3.In the Extensions section (especially for Remote-SSH, which I use for connections), click 'Switch to pre-release Versions.'
4.Restart VSCode.
These steps kept my previous VSCode settings intact. Hope the official team can fix this soon.
p.s. Dont forget to disable the automatic update setting.
```
- 补记：写notes 的时候发现其实中文也可以检索到解决方案 https://zhuanlan.zhihu.com/p/681066025 等等，看来是当时看着玄乎就跳过去了，以后还是得多多注意

## 2024/02/07
#### 加载 llama2
> 是的，我又在尝试加载 llama2 （因为一些意外在几天前下载好的权重被我移除了）
- 这一次索性找了一个现成的 ipynb 直接照抄，但其中还是出现了一些的问题。这里把踩到的坑尽量详尽地列出来
- 我使用的代码来自 https://www.youtube.com/watch?v=AOzMbitpb00 在这个视频下有相应的 github repo 以及 colab 链接，用 Google colab 的免费 GPU 也能够跑动
- 我把代码原封不动复制到服务器上，开始运行，出现报错 ```key error:"llama"``` 我一番搜索，从 https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues/112 得到了一定的启发，发现是服务器上 python 版本过低导致的。那么解决办法就是使用 conda 创建一个新的虚拟环境，保证 py 版本为 3.10 （参照 issue 中解答 ```conda create -n llama python=3.10``` ），随后运行，并安装相应的包即可。
- 幸运的是这种办法安装还是比较快速的？全过程耗时大约在 18 min 左右
#### 补记
非常非常快乐！经过来来回回折腾，我看见了 Llama2 的第一声啼哭！
```
inputs = tokenizer("She is", return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```
得到结果
```
She is the daughter of a very successful businessman. Her father is a very successful businessman and a philanthropist. She is the daughter of a very successful businessman. Her father is a very successful businessman and a philanth
```
### 2024/2/11
#### 如何从 huggingface.co 上加载数据库（被墙情况下）
- 在可以正常连接情况下，请使用官方方法：
```
import datasets
dataset=datasets.load_dataset("NAME_OF_DATASET")
```
- 否则，我尝试了多种方法，最终使用了 Google Colab + 本地 + Google Drive 结合的方式，成功将数据库文件上传到服务器端！
  - 首先，新建 colab 文件，按照正常链接办法下载 dataset，随后执行 ```dataset.save_to_disk("/content/drive/MyDrive/data")``` 注意，需要保证 colab 正确挂载你的 google drive
  - 随后，切换到 google drive 中，下载对应的文件
  - 最后，将保存在本地的文件再次上传到服务器端，使用指令
```
import datasets
dataset=datasets.load_from_disk("PATH")
```
完成加载
### 2024/02/21

### 学习 nginx 中遇见的问题

- 教程 https://www.youtube.com/watch?v=7VAI73roXaY
- 遇见的最严重的 bug：
在教程中重要一步是在本地构建一个简单的服务器，使本地的 index.html 文件可以从 localhost:8080 访问，这就涉及到了配置 nginx.config 的问题。原教程给出的答案是

```
http{
    server {
        listen 8080;
        root /Users/zhizhi/Desktop/mysite;
    }
}

events{
    worker_connections 1024;
}

```

加载后会出现 403 错误，查看 error.log 知道是没有 permission 导致的。于是尝试搜索相关信息，得知需要先在 terminal 中使用 ```ps aux | grep nginx``` 查询 nginx 的用户

```markdown
nobody           12681   0.0  0.0 409340192    240   ??  S     2:37PM   0:00.01 nginx: worker process
root             18995   0.0  0.0 408912160    256   ??  Ss    1:49PM   0:00.01 nginx: master process nginx
zhizhi           27483   0.0  0.0 408626880   1296 s002  S+    2:40PM   0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox nginx
```

随后将 user root； 这一行加入 nginx.config 就可以解决问题。然而事情依然是 403. 随后查到了一篇 CSDN，它告诉我们需要查看 nginx 文档

Specifies the [connection processing](https://nginx.org/en/docs/events.html) *`method`* to use. There is normally no need to specify it explicitly, because nginx will by default use the most efficient method.

| Syntax: | user user [group]; |
| --- | --- |
| Default: | user nobody nobody; |
| Context: | main |

aha！ 由于我们并没有一个组叫 root ，也就无法找到一个叫做 root - root 的用户来授予访问权限。因此，我们需要改成 user root admin; 即可解决问题

```markdown
user root admin;
http{
    server {
        listen 8080;
        root /Users/zhizhi/Desktop/mysite;
    }
}

events{
    worker_connections 1024;
}
```

Hooray!!!